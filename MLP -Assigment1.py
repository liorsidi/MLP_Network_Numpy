
import numpy as np
from mnist import MNIST
import random
#from matplotlib import pyplot as plt
# import numpy.ma as ma
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

print_Debug = False
rand_seed=43

def print_liniar_Cache(cache):
    A,W,b=cache["A"],cache["W"],cache["b"]
    print("A: [{0},{1}] , W: [{2},{3}], B: {4}".format(A.shape[0], A.shape[1], W.shape[0], W.shape[1], len(b)))
    return 0

def initialize_parameters(layer_dims):
# input: an array of the dimensions of each layer in the network (layer 0 is the size of the
# flattened input, layer L is the output sigmoid)
# output: a dictionary containing the initialized W and b parameters of each layer
# (W1â€¦WL, b1â€¦bL).
# Hint: Use the randn and zeros functions of numpy to initialize W and b, respectively

    #set random seed for getting same results each training
    np.random.seed(rand_seed)
    parameters = {}
    for i in range(1,len(layer_dims)):
        prev_dim= layer_dims[i-1]
        curnt_dim= layer_dims[i]
        W = np.random.randn(curnt_dim, prev_dim)* init_Factor
        # W = np.random.uniform(-1,1,(curnt_dim, prev_dim))*init_Factor
        #W = np.random.normal(0.0, 0.25, (curnt_dim, prev_dim)) #* init_Factor
        b = np.zeros((curnt_dim, 1))
        parameters['W' + str(i)] = W
        parameters['b' + str(i)] = b

        if print_Debug:
            print("W" + str(i) +"': "+ str(W))
            print("b" + str(i) +"': "+ str(b))

    return parameters

def linear_forward(A, W, b):
# Description:
# Implement the linear part of a layer's forward propagation.
# input:
# A â€“ the activations of the previous layer
# W â€“ the weight matrix of the current layer (of shape [size of current layer, size of
# previous layer])
# B â€“ the bias vector of the current layer (of shape [size of current layer, 1])
# Output:
# Z â€“ the linear component of the activation function (i.e., the value before applying the
# non-linear function)
# linear_cache â€“ a dictionary containing A, W, b and Z (stored for making the
# backpropagation easier to compute)

    if print_Debug:
        print("linear_forward:")
        print("A: [{0},{1}] , W: [{2},{3}], B: {4}".format(A.shape[0], A.shape[1], W.shape[0], W.shape[1], len(b)))

    Z = W.dot(A) + b
    linear_cache = dict(A = A, W = W, b = b)
    return Z, linear_cache


def sigmoid(Z):
# Input:
# Z â€“ the linear component of the activation function
# Output:
# A â€“ the activations of the layer
# activation_cache â€“ returns Z, which will be useful for the backpropagation
    A = 1.0 / (1.0 + np.exp(-Z))
    cache = Z
    return A, cache


def relu(Z):
# Input:
# Z â€“ the linear component of the activation function
# Output:
# A â€“ the activations of the layer
# activation_cache â€“ returns Z, which will be useful for the backpropagation

    A = np.maximum(0, Z)
    cache = Z
    return A, cache

def linear_activation_forward(A_prev, W, B, activation):

# Description:
# Implement the forward propagation for the LINEAR->ACTIVATION layer
# Input:
# A_prev â€“ activations of the previous layer
# W â€“ the weights matrix of the current layer
# B â€“ the bias vector of the current layer
# Activation â€“ the activation function to be used (a string, either â€œsigmoidâ€ or â€œreluâ€)
# Output:
# A â€“ the activations of the current layer
# linear_cache â€“ the dictionary generated by the linear_forward function

    A_next, linear_cache  = linear_forward(A_prev,W,B)
    a_cache = linear_cache['A']
    if(activation=="sigmoid"):
        A_next,a_cache =sigmoid(A_next)
    elif(activation=="relu"):
        A_next,a_cache = relu(A_next)
    else:
        print("No activation - apply linear")

    return A_next, (linear_cache,a_cache)

def L_model_forward(X, parameters):

    # Description:
    # Implement forward propagation for the [[[LINEAR->RELU]*(L-1)]]        ->  last [ LINEAR->SIGMOID ]
    # computation
    # Input:
    # X â€“ the data, numpy array of shape (input size, number of examples)
    # parameters â€“ the initialized W and b parameters of each layer
    # Output:
    # AL â€“ the last post-activation value
    # caches â€“ a list of all the cache objects generated by the linear_forward function

    A_prev = np.array(X, copy = True)
    caches = []
    layers_len = int(len(parameters.keys())/2)

    for l_name in range(1,layers_len+1):
        W=parameters['W' + str(l_name)]
        b=parameters['b' + str(l_name)]

        if l_name == layers_len:
            A_prev, cache = linear_activation_forward(A_prev, W, b, 'sigmoid')
        else:
            A_prev, cache = linear_activation_forward(A_prev, W, b, 'relu')
        if print_Debug:
            print_liniar_Cache(cache[0])
        caches.append(cache)

    return A_prev, caches

def compute_cost(AL, Y):

# Description:
# Implement the cost function defined by equation
# ğ‘ğ‘œğ‘¡ğ‘¢ = âˆ’
# 1
# ğ‘š
# âˆ— âˆ‘ [(ğ‘§ ğ‘– âˆ— log(ğ´ğ¿)) + ((1 âˆ’ ğ‘§ ğ‘– ) âˆ— (1 âˆ’ ğ´ğ¿))]
# ğ‘š
# 1
# (see the slides of the first
# lecture for additional information if needed).
# Input:
# AL â€“ probability vector corresponding to your label predictions, shape (1, number of
# examples)
# Y â€“ the labels vector (i.e. the ground truth)
# Output:
# cost â€“ the cross-entropy cost

    y_shape = Y.shape[1]
    if print_Debug:
        print("Y: [{0},{1}]".format(Y.shape[0], Y.shape[1]))
        print("AL: [{0},{1}]".format(AL.shape[0], AL.shape[1]))

    cost = (-1.0/y_shape)*\
           (
                   np.dot(Y,  np.log(AL).T) +
                   np.dot((1.0-Y),np.log((1.0-AL).T))
            )

    return cost[0][0]

def linear_backward(dZ, cache):

# Description:
# Implements the linear part of the backward propagation process for a single layer
# Input:
# dZ â€“ the gradient of the cost with respect to the linear output of the current layer (layer l)
# cache â€“ tuple of values (A_prev, W, b) coming from the forward propagation in the current
# layer
# Output:
# dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1),
# same shape as A_prev
# dW -- Gradient of the cost with respect to W (current layer l), same shape as W
# db -- Gradient of the cost with respect to b (current layer l), same shape as b

    A_prev, W, b = cache["A"], cache["W"], cache["b"]
    data_shape = A_prev.shape[1]
    # dW = 1.0 / data_shape * np.dot(dZ, A_prev.T)
    # db = 1.0 / data_shape * np.sum(dZ, axis=1, keepdims=True)
    dW =  np.dot(dZ, A_prev.T) / float(data_shape)
    db =  np.sum(dZ, axis=1, keepdims=True) / float(data_shape)
    dA_prev = np.dot(W.T, dZ)
    linear_cache = dict(A = dA_prev, W = dW, b = db)

    return linear_cache

def linear_activation_backward(dA, cache, activation):

# Description:
# Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first
# computes dZ and then applies the linear_backward function.
# Some comments:
# ï‚· The derivative of ReLU is ğ‘” â€² (ğ‘¦) = { 1
# ğ‘–ğ‘” ğ‘¦ > 0
# 0 ğ‘œğ‘¢â„ğ‘“ğ‘ ğ‘¥ğ‘–ğ‘¡ğ‘“
# ï‚· The Sigmoid function is ğœ(ğ‘¦) =
# 1
# 1+ğ‘’ âˆ’ğ‘¥ and its derivative is
# ğœ â€² (ğ‘¦) = ğœ(ğ‘¦) âˆ— (1 âˆ’ ğœ(ğ‘¦))
# ï‚· You should use the activations cache created earlier for the calculation of the
# activation derivative and the linear cache should be fed to the linear_backward
# function
# Input:
# dA â€“ post activation gradient of the current layer
# cache â€“ contains both the linear cache and the activations cache
# Output:
# dA_prev â€“ Gradient of the cost with respect to the activation (of the previous layer l-1),
# same shape as A_prev
# dW â€“ Gradient of the cost with respect to W (current layer l), same shape as W
# db â€“ Gradient of the cost with respect to b (current layer l), same shape as b


    linear_cache, activation_cache = cache
    dZ = dA
    if activation == 'relu':
        dZ = relu_backward(dA, activation_cache )

    if activation == 'sigmoid':
        dZ = sigmoid_backward(dA, activation_cache )

    linear_cache_backwards= linear_backward(dZ, linear_cache)
    return linear_cache_backwards


def relu_backward(dA, cache):
    """
    Implement the backward propagation for a single RELU unit.

    Arguments:
    dA -- post-activation gradient, of any shape
    cache -- 'Z' where we store for computing backward propagation efficiently

    Returns:
    dZ -- Gradient of the cost with respect to Z
    """

    dZ = np.array(dA, copy=True)
    dZ[cache <= 0] = 0
    dZ[cache > 0] = 1
    return dA*dZ

def sigmoid_backward(dA, activation_cache):

# Description:
# Implements backward propagation for a sigmoid unit
# Input:
# dA â€“ the post-activation gradient
# activation_cache â€“ contains Z (stored during the forward propagation)
# Output:
# dZ â€“ gradient of the cost with respect to Z
    alpha = 1.0/(1.0 + np.exp(-1.0*activation_cache))
    dZ = alpha * (1.0-alpha)
    return dA * dZ

def L_model_backward(AL, Y, caches):

# Description:
# Implement the backward propagation process for the entire network.
# Some comments:
# ï‚· The backpropagation for the Sigmoid should be done separately (because there is
# only one like it), and the process for the ReLU layers should be done in a loop
# ï‚· The derivative for the output of the softmax layer can be calculated using:
# dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
# Input:
# AL â€“ the probabilities vector, the output of the forward propagation (L_model_forward)
# Y â€“ the true labels vector (the â€œground truthâ€ â€“ true classifications)
# Caches â€“ list of caches containing for each layer: a) the linear cache; b) the activation cache
# Output:
# Grads â€“ a dictionary with the gradients
# grads["dA" + str(l)] = ...
# grads["dW" + str(l)] = ...
# grads["db" + str(l)] = ...
    if print_Debug: print("--------------------backwards-------------")
    Grads = {}
    #softmax
    l = len(caches)
    if print_Debug:   print("Y: " , Y)
    dAL = - (np.divide(Y, AL) - np.divide(1.0 - Y, 1.0 - AL))
    linear_cache_backwards =linear_activation_backward(dAL, caches[l-1],'sigmoid')
    if print_Debug:    print_liniar_Cache(linear_cache_backwards)
    Grads["dA" + str(l)], Grads["dW" + str(l)], Grads["db" + str(l)] = linear_cache_backwards["A"],linear_cache_backwards["W"],linear_cache_backwards["b"]
    for l_ in reversed(range(l-1)):
        linear_cache_backwards = linear_activation_backward(Grads["dA" + str(l_+2)], caches[l_],'relu')
        if print_Debug: print_liniar_Cache(linear_cache_backwards)
        Grads["dA" + str(l_+1)], Grads["dW" + str(l_+1)], Grads["db" + str(l_+1)] = linear_cache_backwards["A"],linear_cache_backwards["W"],linear_cache_backwards["b"]
    return Grads

def Update_parameters(parameters, Grads, learning_rate):

# Description:
# Updates parameters using gradient descent
# Input:
# parameters â€“ a python dictionary containing the DNN architectureâ€™s parameters
# grads â€“ a python dictionary containing the gradients (generated by L_model_backward)
# learning_rate â€“ the learning rate used to update the parameters (the â€œalphaâ€)
# Output:
# parameters â€“ the updated values of the parameters object provided as input

    Params_len=int(len(parameters.keys())/2)

    for l_name in range(1,Params_len+1):
        W = parameters['W' + str(l_name)]
        b = parameters['b' + str(l_name)]

        dW = Grads["dW" + str(l_name)]
        db = Grads["db" + str(l_name)]

        parameters['W' + str(l_name)] = W-learning_rate*dW
        parameters['b' + str(l_name)] = b-learning_rate*db

    return parameters


def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations):

# Description:
# Implements a L-layer neural network. All layers but the last should have the ReLU activation
# function, and the final layer will apply the sigmoid activation function. The network should
# only address binary classification.
# Hint: the function should use the earlier functions in the following order: initialize ->
# L_model_forward -> compute_cost -> L_model_backward -> update parameters
# Input:
# X â€“ the input data, a numpy array of shape (height*width , number_of_examples)
# Comment: since the input is in grayscale we only have height and width, otherwise it would
# have been height*width*3
# Y â€“ the â€œrealâ€ labels of the data, a vector of shape (1, number of examples)
# Layer_dims â€“ a list containing the dimensions of each layer, including the input
# Output:
# parameters â€“ the parameters learnt by the system during the training (the same parameters
# that were updated in the update_parameters function).
# costs â€“ the values of the cost function (calculated by the compute_cost function). One value
# is to be saved after each 100 training iterations (e.g. 3000 iterations -> 30 values).

    parameters = initialize_parameters(layers_dims)
    costs = []
    for i in range(num_iterations):
        AL, caches = L_model_forward(X,parameters)

        if i % 100 == 0:
            cost = compute_cost(AL, Y)
            costs.append(cost)
            Grads = L_model_backward(AL, Y, caches) #move this up
            print('iteration ' + str(i+100) + ' cost: ' + str(cost))
            parameters = Update_parameters(parameters, Grads, learning_rate) #move this up
    return parameters, costs


def predict(X, parameters):
    """
    This function is used to predict the results of a  L-layer neural network.

    Arguments:
    X -- data set of examples you would like to label
    parameters -- parameters of the trained model

    Returns:
    p -- predictions for the given dataset X
    """

    preds = []
    # get probas
    probs, caches = L_model_forward(X, parameters)
    # predict between 1 and 0
    for i in range(0, probs.shape[1]):
        if probs[0, i] > 0.5:
            preds.append(1)
        else:
            preds.append(0)
    preds = np.array(preds)

    return preds



def LoadMinst():
    #load mnist dataset

    # load date
    mndata = MNIST('dataset')
    mndata.gz = True
    images, labels = mndata.load_training()

    # convert to np
    np_images = np.array(images)
    np_labels = np.array(labels)
    print("data set size = " ,len(np_labels))

    return np_images, np_labels

def binaries_labels(Y_):
    """
    transform the labeles to binary representation
    :param Y_:
    :return:
    """

    unique_values = np.unique(Y_)
    assert (len(unique_values)==2)

    Y_[np.where((Y_==unique_values[0]))]=0
    Y_[np.where((Y_==unique_values[1]))]=1
    return Y_

if __name__ == "__main__":


    hidden_layers = [20, 7, 5, 1]
    learningRate = 0.009
    Num_Of_Iterations = 3000

    images , labels = LoadMinst() # load data

    hidden_layers = [images.shape[1]] + hidden_layers
    layer_first = images.shape[1]        # input layer size

    ind_8_3 = np.where((labels == 8) | (labels ==3))
    ind_9_7 = np.where((labels == 9) | (labels == 7))
    experiments = dict(exp_8vs3 = ind_8_3,
                       exp_9vs7 = ind_9_7 )
    init_Factors = dict(exp_8vs3 = 0.085,exp_9vs7 = 0.115 )
    for experiment_k, experiment_ind in experiments.items():
        print(experiment_k)
        init_Factor = init_Factors[experiment_k]
        print('init factor - ' + str(init_Factor))
        X_, Y_ = np.take(images,experiment_ind,axis=0)[0] ,np.take(labels,experiment_ind)[0]
        X_ = X_/255.0
        Y_ = binaries_labels(Y_)

        # samples = 52
        # X_, Y_  = X_[:samples,:], Y_[:samples]

        X_train, X_test, y_train, y_test = train_test_split(X_, Y_, test_size = 0.2, random_state = rand_seed)
        y_train = y_train.reshape((1, y_train.shape[0]))
        y_test = y_test.reshape((1, y_test.shape[0]))
        X_train = np.transpose(X_train)
        X_test = np.transpose(X_test)

        parameters, costs = L_layer_model(X_train, y_train, hidden_layers,learningRate,Num_Of_Iterations)
        print('experiments costs')
        print(costs)
        #Plot_Costs(costs)
        preds = predict(X_train, parameters)
        acc_train = accuracy_score(preds, y_train[0])
        preds = predict(X_test, parameters)
        acc_test = accuracy_score(preds, y_test[0])
        print('experiment accuracy')
        print("Train Accurcy: {:.2%} Test Accurcy: {:.2%}".format(acc_train,acc_test))
